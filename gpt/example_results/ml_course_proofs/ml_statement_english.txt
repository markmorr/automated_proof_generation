[{'prompt': 'Consider the classification problem on an arbitrary (measurable) input space $X$ and a binary output space $Y={0,1}$. Given a join data distribution \\mathcal{D} over $X \\times Y$, let $g:X \\rightarrow Y$ denote the Bayes classifier $g(x) := \\text{arg max}_{Y}\\text{Pr}[Y\\vert X=x]$. Define $\\text{ERR}(g):=\\text{Pr}_{(x,y)\\sim \\mathcal{D}}[g(x) \\neq y]$ as the error rate of the Bayes classifier. If the class priors are equal (that is, $\\text{Pr}[Y=0]=\\text{Pr}[Y=1]=1/2$), then: $\\text{ERR}(g) = \\frac{1}{2}-\\frac{1}{4}\\int_{x\\in X}\\Big|f_{1}(x)-f_{0}(x)\\Big|dx$',
  'completion': 'Simply let $p=\x0crac{1}{2}=1-p$, and apply the result from part (i) for the Bayes Error. \n {{begin-eqn}} \n {{eqn | \\mathbb{E}_{x\\sim D\\vert _X}[\\text{min}(\\eta(x),1-\\eta(x))] &= \\frac{1}{2}-\\frac{1}{2}\\mathbb{E}_{x\\sim D\\vert _X}\\bigg|2\\eta(x)-1\\bigg|}} \n {{eqn | &= \\frac{1}{2}-\\frac{1}{2}\\int_{x\\in X}\\bigg|\\eta(x)-(1-\\eta(x))\\bigg|P[x=x]dx}} \n {{eqn | &= \\frac{1}{2}-\\frac{1}{2}\\int_{x\\in X}\\bigg|pf_{1}(x)-(1-p)f_{0}(x)\\bigg|dx}} \n {{eqn | &= \\frac{1}{2}-\\frac{1}{4}\\int_{x\\in X}\\bigg|f_{1}(x)-f_{0}(x)\\bigg|dx}} \n {{end-eqn}}'},
 {'prompt': 'Recall the game of rock-paper-scissors. In this game there are two players $p_1$ and $p_2$, that each choose an element of ${r,p,s}$ as their only move for the game. One can study such games more systematically. For example, this game can be analyzed by examining the possible moves each player can make and representing the outcomes as a payoff matrix $P$. In particular, given that $p_1$ plays move $i \\in {r,p,s}$ and $p_2$ plays move $j \\in {r,p,s}$, the resulting game payoff (from the perspective of $p_1$) is the amount $P_{i,j}$. If the matrix $Q$ is the payoff from the persepctive of player $p_2$, then what is the relationship between $P$ and $Q$?',
  'completion': '$P \\mathop = -Q$'},
 {'prompt': 'Recall the game of rock-paper-scissors. In this game there are two players $p_1$ and $p_2$, that each choose an element of ${r,p,s}$ as their only move for the game. One can study such games more systematically. For example, this game can be analyzed by examining the possible moves each player can make and representing the outcomes as a payoff matrix $P$. In particular, given that $p_1$ plays move $i \\in {r,p,s}$ and $p_2$ plays move $j \\in {r,p,s}$, the resulting game payoff (from the perspective of $p_1$) is the amount $P_{i,j}$. What is the payoff matrix for $p_1$ in the game of rock paper scissors, where winning results in a payoff of $1$, losing payoff of $-1$, and a tie a payoff of $0$?',
  'completion': '$Pay_{p1} \\mathop = \\begin{pmatrix} 0 & -1 & 1 \\ 1 & 0 & -1 \\ -1 & 1 & 0'},
 {'prompt': "Consider a game with the payoff matrix $A \\in {-1,1}^{n \\times n}$ (notice, =1 means win, -2 means lose, and there are no ties in this game). There are two players, $p_1$ and $p_2$, that must choose moves $i$ and $j \\in {1,...,n}$, respectively. Similar to rock-paper-scissors, neither $p_1$ nor $p_2$ knows the toher's choice before making their won choice, but they do have access to the payoff $A$. Following their choices of $i$ and $j$, $p_1$ receives reward $A_{i,j}$. Of course, $p_1$'s objective is to receive maximum possible payoff, and since $p_2$ and $p_1$ are in competition, $p_2$'s objective is for $p_1$ to receive minimum possible payoff. Describe the set of matrices $S \\subset {-1,1}^{n \\times n}$ s.t. $\\forall A \\in S$, there exists some choice of $i \\in {1,...,n}$ that guarantees a win for $p_1$ (no more than two sentences are needed).",
  'completion': "$S$ s.t. there is some row in $A$ with all 1's."},
 {'prompt': 'Recall the RBF kernel with parameter $\\gama > 0$$ which, given $x,x^\\prime \\in \\mathbb{R}^d$, is defined as $K(x,x^\\prime) = \\exp{- \\gamma \\| x-x^\\prime  \\|^2}. Assume that $d=1$. Let \\phi \\mathop = 1. Let \\phi be the feature transform which maps an input $x \\in \\mathbb{R}$ to an infinite dimensional vector whos $i$th ($i$ takes on values 0,1,2,...) coordinate is $(\\phi(x))_i = \\sqrt{\\frac{2^i\\gamma^2}{i!}\\exp{-\\gamma x^2}x^i. We can then define $\\phi(x) \\cdot \\phi(x^\\prime) =  _0\\phi(x)_i \\phi(x^\\prime)_i. Show that $K(x,x^\\prime)= \\phi(x) \\cdot \\phi(x^\\prime)$ for the specific $K$ and \\phi defiend above.',
  'completion': 'Expanding the dot product $\\phi(x) \\cdot \\phi(x^\\prime)$, we get: \n {{begin-eqn}} \n {{eqn | \\phi(x) \\cdot \\phi(x^\\prime) = \\sum^{\\infty}_{i=0} \\left (\\sqrt{\\frac{2^i \\gamma^1}{i!}}\\exp(-\\gamma x^2)x^i \\right )\\left ( \\sqrt{\\frac{2^i \\gamma^1}{i!}}\\exp(-\\gamma (x^\\prime)^2)(x\\prime)^i \\right )}} \n {{eqn | &= \\exp(-\\gamma(x^2 + x^{\\prime 2}))\\sum^\\infty_{i=0}\\frac{2^i\\gamma^i}{i!}(xx^\\prime)^i}} \n {{eqn | &= \\exp(-\\gamma(x^2 + x^{\\prime 2}))\\sum^\\infty_{i=0}\\frac{(2^i\\gamma xx^\\prime)^i}{i!}}} \n {{eqn | &= \\exp(-\\gamma(x^2 + x^{\\prime 2})) \\cdot \\exp(2\\gamma xx^\\prime)}} \n {{eqn | &= \\exp(-\\gamma x^2 - \\gamma x^{\\prime 2} + 2\\gamma(xx^\\prime))}} \n {{eqn | &= \\exp(-\\gamma(x-x^\\prime)^2)}} \n {{eqn | &= \\exp(-\\gamma \\|x - x^\\prime \\|^2)}} \n {{eqn | &= K(x,x^\\prime)}} \n {{end-eqn}}'},
 {'prompt': "Your friend gives you a coin with bias $b$ (that is, tossing the coin turns '1' with probability $b$, and turns '0' with probability $1-b$). You make $n$ independent tosses and get the observation sequence $x-1,...,x_n \\subset {0,1}$. You want to estimate the coin's bias. What is the Maximum Likelihood Estimate (MLE) \\hat{b} given the observations $x_1,...,x_n$? Is your estimate an unbiased estimator of $b$? How about consistent? Justify your answer.",
  'completion': "We need to find the argmax over $b$ value that solves this equation, which is the likelihood of the data: ${n\\choose k }\\prod_{i=1}^{n} b^{x_i}(1-b)^{1-x_i}$ \n Taking the log of this expression, to get the log likelihood (since this won't affect the arg max value): \n $\\log{{n\\choose k }} +\\sum_{i=1}^{n}{x_i\\log{b} + (1 - x_i)\\log(1 - b)}$ \n Taking the derivative and setting it equal to 0 to find a maximum of this function: \n $0 + \\sum_{i=1}^{n}{\\frac{x_i}{b} - \\frac{1 - x_i}{1-b}} = 0$ \n $\\sum_{i=1}^{n}{\\frac{x_i}{b}} = \\sum{\\frac{1 - x_i}{1-b}}$ \n $(1-b)\\sum_{i=1}^{n}{x_i} = (b)\\sum{1 - x_i}$ \\\\ $(1-b)(n\\bar{x}) = (b)(n - n\\bar{x})$ \n $n\\bar{x} - bn\\bar{x} = bn - bn\\bar{x}$ \n $n\\bar{x} = bn$ \n $\\bar{x} = b$ \n $\\hat{b} = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}{x_i}$ \n To check if an estimate is biased, we need to check if the expectation of our estimate is different from the true parameter value: $\\mathbb{E}[\\hat{b}] = \\frac{\\sum_{i=1}^n{\\mathbb{E}[x_i]}}{n} = \\frac{nb}{n} = b$ \n Thus, $\\mathbb{E}[\\hat{b}] = b$ \n And they are equivalent, so their estimate is unbiased."},
 {'prompt': "Your friend gives you a coin with bias $b$ (that is, tossing the coin turns '1' with probability $b$, and turns '0' with probability $1-b$). You make $n$ independent tosses and get the observation sequence $x-1,...,x_n \\subset {0,1}$. Derive a simple expression for the variance of this coin. What is the MLE for the coin's variance?",
  'completion': '{{being-eqn}} \n {{eqn | Var(x) = \\mathbb(E)\\vert x^2 \\vert - \\mathbb{E} \\vert x \\vert^2}} \n {{eqn | &= 1^2 \\times b + 0^2 \\times (1-b)^2 - b^2}} \n {{eqn | &= b(1-b)}} \n {{end-eqn}} \n Treat $Var(X)$ as a function of $x$. \n We know $g(\\theta_{ML} = g(\\theta)_{ML}$. \n Consider $x$ as the $\\theta$ and $Var(x)$ as $g(\\theta)$. \n We have \n {{begin-eqn}} \n {{eqn ! Var(x)_{ML} = Var(\\hat{x}){ML}=\\hat{b}(1-\\hat{b}).'},
 {'prompt': "Your friend gives you a coin with bias $b$ (that is, tossing the coin turns '1' with probability $b$, and turns '0' with probability $1-b$). You make $n$ independent tosses and get the observation sequence $x-1,...,x_n \\subset {0,1}$. Your friend reveals to you that the coin was minted from a faulty press that biased it towards 1. Suppose the model for the faulty bias is given by the following distribution: having this extra knowledge, what is the best estimate for the coin's bias 4b$ given the observation sequence? That is, compute $\\text{arg max}_b P[b \\vert x_1,...,x_n]. When does MAP estimate equals MLE?",
  'completion': 'Using Bayes rule, we rewrite the conditional probability as follows: \n {{begin-eqn}} \n {{eqn | P(b \\vert x-1,...,x_n) = \\frac{P(x_1,...,x_n \\vert b) \\times P(b)}{P(x_1,...,x_n)}}} \n {{end-eqn}} \n Because the denominator ("evidence") is constant w.r.t $b$, we have the following \n {{begin-eqn}} \n {{eqn | \\text{arg max}_b P(b \\vert x_1,...,x_n) = \\text{arg max}_b \\log{P(x_1,...,x_n \\vert b} + \\log{P(b)}}} \n {{end-eqn}} \n Notice $P(b)$ is defined as a line $P(b) = 2b$ for $0 \\le b \\le 1$. \n We again take the derivative of the function on the RHS with respect to $b$, set the derivative to 0, and solve for $b$. \n The derivative is \n {{begin-eqn}} \n {{eqn | \\frac{\\partial P(b \\vert x_1,...,x_n)}{\\partial b} = \\sum^n_{i=1} \\left ( \\frac{x-i}{b} - \\frac{1-x_i}{1-b} \\right ) + \\frac{1}{b}}} \n {{end-eqn}} \n Setting the equation to 0 and solve for $b$, we have: \n {{begin-eqn}} \n {{eqn | \\sum^n_{i=1} \\left ( \\frac{x_i}{b} - \\frac{1-x_i}{1-b} + \\frac{1}{b}=0}} \n {{eqn | \\frac{1+\\sum^n_{i=1}x_i}{b} = \\frac{n-\\sum^n_{i=1}x_i}{1-b}}} \n {{eqn | \\frac{n-\\sum^n_{i=1}x_i-b\\frac{n-\\sum^n_{i=1}x_i + 1-b = bn - b\\frac{n-\\sum^n_{i=1}x_i}} \n {{eqn | b = \\frac{1 + \\frac{n-\\sum^n_{i=1}x_i}{n+1}}} \n {{end-eqn}} \n If $P(b) is constant, that is $b$ is drawn from a uniform distribution, its derivative would be 0, and then the MAP estimate reduces to MLE.'},
 {'prompt': 'If for each iteration $t$ there is some $\\gamma-t > 0$ such that $\\epsilon_t=\\frac{1}{2}-\\gamma_t$ (that is, assuming that at each iteration the error of the classifier $f_t$ is just $\\gamma_t$ better than random guessing), then error of the aggregate classifier $\\text{err}(g) := \\frac{1}{m} \\sum_i \\textbf{1}[y_i \\ne \\text{sign}(g(x_i))] \\le \\exp(-2 \\sum^T_{t=1}\\gamma^2_t).$ That is, the error of the aggregate classifier $g$ decreases exponentially fast with the number of combinations $T$! Show that $Z_t=2\\sqrt{\\epsilon_t(1-\\epsilon_t)}.$'},
 {'prompt': 'If for each iteration $t$ there is some $\\gamma-t > 0$ such that $\\epsilon_t=\\frac{1}{2}-\\gamma_t$ (that is, assuming that at each iteration the error of the classifier $f_t$ is just $\\gamma_t$ better than random guessing), then error of the aggregate classifier $\\text{err}(g) := \\frac{1}{m} \\sum_i \\textbf{1}[y_i \\ne \\text{sign}(g(x_i))] \\le \\exp(-2 \\sum^T_{t=1}\\gamma^2_t).$ That is, the error of the aggregate classifier $g$ decreases exponentially fast with the number of combinations $T$! We have that $\\text{err}(g) \\prod_t 2 \\sqrt{\\epsilon_t(1-\\epsilon_t)}$, now show that: $\\prod_t 2\\sqrt{\\epsilon_t(1-\\epsilon_t)}=\\prod_t \\sqrt{1-4\\gamma62_t} \\le \\exp(-2\\sum_t \\gamma^2_t).$ Thus establishing that $\\text{err}(g) \\le \\exp(-2\\sum_t \\gamma^2_t).$'},
 {'prompt': 'If for each iteration $t$ there is some $\\gamma-t > 0$ such that $\\epsilon_t=\\frac{1}{2}-\\gamma_t$ (that is, assuming that at each iteration the error of the classifier $f_t$ is just $\\gamma_t$ better than random guessing), then error of the aggregate classifier $\\text{err}(g) := \\frac{1}{m} \\sum_i \\textbf{1}[y_i \\ne \\text{sign}(g(x_i))] \\le \\exp(-2 \\sum^T_{t=1}\\gamma^2_t).$ That is, the error of the aggregate classifier $g$ decreases exponentially fast with the number of combinations $T$! Show that the error of the aggregate lcassifier $g$ is upper bounded by the product of $Z_t: \\text[err}(g) \\le \\prod_t Z_t$.'}]